

Cloud computing is the practice of using remote servers on the internet for carrying out a task, rather than using our own computers/servers

aws has more than 200 services
servers --->virtual machines

cloud to on primieses --->cloud repatriation
cloud advantages :
cost cutting of servers , maintenance of servers, to resuce the wastage of servers.
need to monitor servers 24/7 , powerloss , dedicated resources, need to protect from hacks ,data centres

developer-->creates app----> buys some servers and put the application over the there--->and interet is provided to the servers so that end users can able to download/use 

advantages of cloud computing--->
1.Little or no investment
2.More focus on app development
3.Requires less  workforce


cloud computing models
1.Deployement models--->1.public cloud--->2.private cloud---->3.Hybrid cloud
2.Service models--->1.Infrastructure as a service-->2.Platform as a service--->3.software as a service

public cloud is defined as computing services offfered by third party providers over the public internet, making them available to anyone who wants to use or purchase them(you allow them to host more applications on the same server)

private cloud refers to a model of cloud computing where IT services are provisioned over private IT infrastructure for the dedicated use of a single organization


1.Infrastructure as a serve



IAM in aws is a aws service , it is sloving problem of authentication and authorization

IAM 
user-->authentication part(create aws user)
policies-->granting some required permissions
Group-->to reduce efforts of applying policies and permissions
roles-->it has username & password but these are temporary-->2 communicate between 2 aws accounts-->application is in private cloud and DB is in aws in order to get data from DB and give it to customer we create roles-->application is accessing something from aws


creating user
Console sign-in URL

https://825624152408.signin.aws.amazon.com/console
User name

kiran_test_user
Console password

uveGu0%l--->Saikiran@123
Hide


{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Action": [
                "s3:*",
                "s3-object-lambda:*"
            ],
            "Resource": "*"
        }
    ]
}
* indicate all/everything access

EC2 ---> Elastic cloud compute (asking aws to give me virtual machine on your cloud platform which is elastic in nature)asking aws(public cloud provider to give you a virtual server which is elastic in nature
compute(vertual server)i.e cpu,ram,disk
cloud--->cloud machine, elastic --> prefix to some of aws service(increase/decrease resources)


Types of Ec2 instances :
General
Compute optimized
memory
storage
Accelarated

Region-->aws has data centres across the word each counrty ,for eg region : india , in india multiple data centres are present high availability instance

lattence-->time taken for the request to reach the application, and for the application/server to send the response back to you(low lattency)


Virtual private cloud (VPC)
size of vpc can be xplained with ipaddress range
subnet--splitting ipadress range for sub projects

load balancer--->forwards the requests based upon the load

how wiil the request goes from public subnet to

internet gateway>>public subnet>> 
load balancers>>routers(route table)>>security groups

NACL--automation

NAT gateways-->masking of ip address (help to download from internet)

wthin the vpc there will be public and private subnets

security in subnet level--->NACL

SG-->1.inbound-->2.outbound
NACL-->network access control list
security group            NACL
1.instance level    1 .subnet level 
2.used for only allowing  allow+deny traffic


route 53--->Route 53 on AWS provides dns as a service
DNS --->Domain name system
We don't use ip addresses instead we use domain name
DNS service maps the domain name with the ip adresss
when you create the load balancer, aws gives the loadbalancer a ip address.


What is Amazon S3?

Simple Storage Service is a scalable and secure cloud storage service provided by Amazon Web Services (AWS). It allows you to store and retrieve any amount of data from anywhere on the web.

What are S3 buckets?

S3 buckets are containers for storing objects (files) in Amazon S3. Each bucket has a unique name globally across all of AWS. You can think of an S3 bucket as a top-level folder that holds your data.

Why use S3 buckets?

S3 buckets provide a reliable and highly scalable storage solution for various use cases. They are commonly used for backup and restore, data archiving, content storage for websites, and as a data source for big data analytics.

Key benefits of S3 buckets

S3 buckets offer several advantages, including:

Durability and availability: S3 provides high durability and availability for your data.
Scalability: You can store and retrieve any amount of data without worrying about capacity constraints.
Security: S3 offers multiple security features such as encryption, access control, and audit logging.
Performance: S3 is designed to deliver high performance for data retrieval and storage operations.
Cost-effective: S3 offers cost-effective storage options and pricing models based on your usage patterns.
Creating and Configuring S3 Buckets
Creating an S3 bucket

To create an S3 bucket, you can use the AWS Management Console, AWS CLI (Command Line Interface), or AWS SDKs (Software Development Kits). You need to specify a globally unique bucket name and select the region where you want to create the bucket.

Choosing a bucket name and region

The bucket name must be unique across all existing bucket names in Amazon S3. It should follow DNS naming conventions, be 3-63 characters long, and contain only lowercase letters, numbers, periods, and hyphens. The region selection affects data latency and compliance with specific regulations.

Bucket properties and configurations

Versioning: Versioning allows you to keep multiple versions of an object in the bucket. It helps protect against accidental deletions or overwrites.
Bucket-level permissions and policies

Bucket-level permissions and policies define who can access and perform actions on the bucket. You can grant permissions using IAM (Identity and Access Management) policies, which allow fine-grained control over user access to the bucket and its objects.

Uploading and Managing Objects in S3 Buckets
Uploading objects to S3 buckets

You can upload objects to an S3 bucket using various methods, including the AWS Management Console, AWS CLI, SDKs, and direct HTTP uploads. Each object is assigned a unique key (name) within the bucket to retrieve it later.

Object metadata and properties

Object metadata contains additional information abouteach object in an S3 bucket. It includes attributes like content type, cache control, encryption settings, and custom metadata. These properties help in managing and organizing objects within the bucket.

File formats and object encryption

S3 supports various file formats, including text files, images, videos, and more. You can encrypt objects stored in S3 using server-side encryption (SSE). SSE options include SSE-S3 (Amazon-managed keys), SSE-KMS (AWS Key Management Service), and SSE-C (customer-provided keys).

Lifecycle management

Lifecycle management allows you to define rules for transitioning objects between different storage classes or deleting them automatically based on predefined criteria. For example, you can move infrequently accessed data to a lower-cost storage class after a specified time or delete objects after a certain retention period.

Multipart uploads

Multipart uploads provide a mechanism for uploading large objects in parts, which improves performance and resiliency. You can upload each part in parallel and then combine them to create the complete object. Multipart uploads also enable resumable uploads in case of failures.

Managing large datasets with S3 Batch Operations

S3 Batch Operations is a feature that allows you to perform bulk operations on large numbers of objects in an S3 bucket. It provides an efficient way to automate tasks such as copying objects, tagging, and restoring archived data.

Advanced S3 Bucket Features
S3 Storage Classes

S3 offers multiple storage classes, each designed for different use cases and performance requirements:

Screenshot 2023-07-06 at 7 16 51 PM

S3 Replication

S3 replication enables automatic and asynchronous replication of objects between S3 buckets in different regions or within the same region. Cross-Region Replication (CRR) provides disaster recovery and compliance benefits, while Same-Region Replication (SRR) can be used for data resilience and low-latency access.

S3 Event Notifications and Triggers

S3 event notifications allow you to configure actions when specific events occur in an S3 bucket. For example, you can trigger AWS Lambda functions, send messages to Amazon Simple Queue Service (SQS), or invoke other services using Amazon SNS when an object is created or deleted.

S3 Batch Operations

S3 Batch Operations allow you to perform large-scale batch operations on objects, such as copying, tagging, or deleting, across multiple buckets. It simplifies managing large datasets and automates tasks that would otherwise be time-consuming.

Security and Compliance in S3 Buckets
S3 bucket security considerations

Ensure that S3 bucket policies, access control, and encryption settings are appropriately configured. Regularly monitor and audit access logs for unauthorized activities.

Data encryption at rest and in transit

Encrypt data at rest using server-side encryption options provided by S3. Additionally, enable encryption in transit by using SSL/TLS for data transfers.

Access logging and monitoring

Enable access logging to capture detailed records of requests made to your S3 bucket. Monitor access logs and configure alerts to detect any suspicious activities or unauthorized access attempts.

S3 Bucket Management and Administration
S3 bucket policies

Create and manage bucket policies to control access to your S3 buckets. Bucket policies are written in JSON and define permissions for various actions and resources.

S3 access control and IAM roles

Use IAM roles and policies to manage access to S3 buckets. IAM roles provide temporary credentials and fine-grained access control to AWS resources.

S3 APIs and SDKs

Interact with S3 programmatically using AWS SDKs or APIs. These provide libraries and methods for performing various operations on S3 buckets and objects.

Monitoring and logging with CloudWatch

Utilize Amazon CloudWatch to monitor S3 metrics, set up alarms for specific events, and collect and analyze logs for troubleshooting and performance optimization.

S3 management tools

AWS provides multiple management tools, such as the AWS Management Console, AWS CLI, and third-party tools, to manage S3 buckets efficiently and perform operations like uploads, downloads, and bucket configurations.

Troubleshooting and Error Handling
Common S3 error messages and their resolutions

Understand common S3 error messages like access denied, bucket not found, and exceeded bucket quota. Troubleshoot and resolve these errors by checking permissions, bucket configurations, and network connectivity.

Debugging S3 bucket access issues

Investigate and resolve issues related to access permissions, IAM roles, and bucket policies. Use tools like AWS CloudTrail and S3 access logs to identify and troubleshoot access problems.

Data consistency and durability considerations

Ensure data consistency and durability by understanding S3's data replication and storage mechanisms. Verify that data is correctly uploaded, retrieve objects using proper methods, and address any data integrity issues.

Recovering deleted objects

If an object is accidentally deleted, you can often recover it using versioning or S3 event notifications. Additionally, consider enabling Cross-Region Replication (CRR) for disaster recovery scenarios.


GIT Commit: 
1.git clone "RepoName/https link"
2.cd reponame
3.Vim foldername
4.git add
5.git commit - "message"
6.git push

AWS codecommit Disadvantages
1.Features
2.AWS Restricted
3.Less integrations with services outside AWS